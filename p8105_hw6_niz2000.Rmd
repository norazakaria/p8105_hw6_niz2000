---
title: "p8105_hw6_niz2000"
author: "Nora Zakaria"
date: "11/25/2019"
output: github_document
---

## Load Tidyverse, Modelr, and Class Datasets

```{r}
library(tidyverse)
library(p8105.datasets)
library(modelr)
```


# Problem 1

## Load and Tidy the Birthweight Data

The birthweight data was gathered to understand the effects of several variables on a child’s birthweight.

```{r warning=FALSE}
birthweight = 
  read_csv(file = "./data/birthweight.csv") %>%
  janitor::clean_names() %>%
  mutate(
      babysex = as.factor(babysex),
      babysex = fct_recode(babysex,
                  "male" = "1", "female" = "2"),
    frace = as.factor(frace),
      frace = fct_recode(frace,
                  "white" = "1", "black" = "2", "asian" = "3", 
                  "Puerto Rican" = "4", "Other" = "8", "Unknown" = "9"),
    malform = as.factor(malform),
      malform = fct_recode(malform,
                  "absent" = "0", "present" = "1"),             
    mrace = as.factor(mrace),
      mrace = fct_recode(mrace,
                  "white" = "1", "black" = "2", "asian" = "3", 
                  "Puerto Rican" = "4", "Other" = "8"))
birthweight
```

The four categorical variables in the dataset: "babysex," "frace," "malform," and "mrace," were converted from numeric to factor variables. These factors were also recoded in order for variable codes to be more descriptive. For example in the babysex variable, the factors now display "male" and "female," opposed to 1 and 2. There are `r nrow(birthweight)` observations and `r ncol(birthweight)` variables in the birthweight dataset.

### Test for Missing Values

After tidying, the missing_test dataframe was created to detect if there are any missing values in the dataset.

```{r warning=FALSE}
missing_test = birthweight %>%
  summarise_all((funs(sum(is.na(.))))) %>%
  knitr::kable(digits = 3, format = 'pandoc', caption = "Test for Missing Values") 
missing_test
```

As we see in the missing_test outputted table, there are 0 missing values in each of the variable columns, indicating that there are no missing values in the birthweight dataset.


## Regression Model for Birthweight

In this linear regression model for the birthweight (bwt) outcome, predictors include mother's pre-pregnancy BMI (ppbmi), mother’s weight gain during pregnancy in pounds (wtgain), mother's age at delivery in years (momage), the number of live births prior to this pregnancy (parity), and the average number of cigarettes smoked per day during pregnancy (smoken). These variables were selected based on previous academic literature findings on the factors that underly birthweight, including information from the World Health Organization and Stanford University. In addition to mother's starting weight, weight gain during preganancy, and age, birth order and behavioral factors such as smoking all contribute to the birthweight of a newborn. 

```{r Fit Linear Model}
fit = lm(bwt ~ ppbmi + wtgain + momage + parity + smoken, data = birthweight)
fit

fit_summary_table = fit %>%
  broom::tidy() %>%
  knitr::kable(digits = 3, format = 'pandoc', caption = "Fit Linear Model Statistics") 
fit_summary_table
```

The linear model summary table provides the regression coefficients, standard error, test statistic, and p-value for all predictors in our model. All p-values were less than 0.05, apart from the "parity" variable, therefore "parity" is the only predictor in our model that is not statistically significant. Additionally, our regression coefficients for "parity" and "smoken" are negative, therefore demonstrating a negative linear relationship between the predictor and our birthweight outcome, with the remaining variables demonstrating a positive linear association. 


## Plot of Model Residuals and Fitted Values

Residuals and fitted values can be examined as diagnostics to identify model fit. In linear regression, residuals measure the difference between the observed value of the outcome and the predicted value of the outcome. Fitted values, or predictions, model the values that come out of the estimation process that the fit model outputs.

```{r Model Plot}
modelr::add_predictions(birthweight, fit)
modelr::add_residuals(birthweight, fit)

birthweight %>%
  modelr::add_residuals(fit) %>%
  modelr::add_predictions(fit) %>%
  ggplot(aes(x = pred, y = resid)) +
    geom_point(aes(color = "viridis"), show.legend = FALSE) +
    labs(
      title = "Model Residuals Against Fitted Values",
      x = "Fitted Values",
      y = "Residuals")  +
    viridis::scale_color_viridis(discrete = TRUE) 
```

The points on the scatterplot of residuals against fitted values are concentrated around fitted values from 2800 to 33000, and residual values evenly spread around 0 from -1000 to 1000. As the residuals vary evenly above and below the 0 line, this suggest that the assumption that the relationship is linear may be reasonable. However, there do appear to be outliers in the plot. 


## Comparing Fit Models

Next, the "fit" model will be compared to two additional models:

* The "main_effects_fit" model, including length at birth and gestational age as predictors; and
* The "interaction_fit" model, including head circumference, length at birth, sex, and all interacting terms as predictors.


### Predictors: Length at Birth and Gestational Age Main Effects

```{r Main_Effects_Fit Model}
main_effects_fit = lm(bwt ~ blength + gaweeks, data = birthweight)
main_effects_fit 
```

As there are no interaction terms, the "length_age_fit" regression model outputs an intercept, and two coefficients for each predictor, length at birth and gestational age.


### Predictors: Head Circumference, Length, Sex, and All Interacting Terms

```{r Interaction_Fit Model}
interaction_fit = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + 
  blength*babysex + bhead*blength*babysex, data = birthweight)
interaction_fit
```

As there are interaction terms, the "interaction_terms_fit" regression model outputs an intercept, and seven coefficients. There are three coefficients for for each head circumference, length, and sex predictor, three for interaction terms between two of the predictors, and one coefficient for the interaction term between all three predictors.


### Comparing Cross-Validated Prediction Errors

In order to compare the "fit," "main_effects_fit," and "interaction_fit" models, cross-validation will be emplored. Cross validation provides a way to compare the predictive performance of competing methods. In this case, models will be compared based on root mean squared errors (RMSEs). 

```{r}
cv_df = 
  crossv_mc(birthweight, 100) %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

The "crossv_mc" automates elements of the cross-validation process by reforms the testing split multiple times, and stores the datasets using list columns. This step is coerced to a dataframe called "cv_df," and can be treated like a dataframe moving forward. 

```{r Cross Validation}
cv_df = cv_df %>%
  mutate(
    fit = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    main_effects_fit  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    interaction_fit = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + 
        bhead*babysex + blength*babysex + bhead*blength*babysex, data = birthweight))) %>%
  mutate(
    rmse_fit = map2_dbl(fit, test, ~rmse(model = .x, data = .y)),
    rmse_main_effects = map2_dbl(main_effects_fit, test, ~rmse(model = .x, data = .y)),
    rmse_interaction = map2_dbl(interaction_fit, test, ~rmse(model = .x, data = .y)))
 
cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rsme_") %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse, color = model)) +
    geom_violin() +
    labs(
      title = "Cross-Validated Prediction Errors",
      x = "Model",
      y = "RMSE") +
    viridis::scale_color_viridis(discrete = TRUE) 
```

The Cross-Validated Prediction Errors plot provides a violin plot of RMSE values for each of the three models: the fit model, the main effects model, and the interaction terms model. From these violin plots of variance in prediction error, we can compare prediction error distributions across models. Based on these results, the interaction model (interaction_fit) is the best model with RMSE values around 285, as it has the lowest RMSE relative to the other two models, which both have high RMSE around 330 and are similar to each other in spread. 


# Problem 2

## Load 2017 Central Park NOAA Weather Data

The data used for this question is from the NOAA National Climatic Data Center, for Central Park, NYC.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
weather_df
```

There are `r nrow(weather_df)` observations and `r ncol(weather_df)` variables in the NOAA weather dataset for Central Park. Key variables for subsequent analyses include "tmin" and "tmax."


## Central Park Weather Bootstrap Data Frame

```{r Weather Bootstrap}
weather_bootstrap = weather_df %>%
  modelr::bootstrap(n = 5000) %>%
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)))
 weather_bootstrap
```

The resulting weather_bootstrap dataframe consists of 5,000 bootstrap samples. The underlying simple linear regression model displays "tmax" as the response, and "tmin" as the predictor of interest.


## Distribution of log(β̂ 0∗β̂ 1)

```{r Log(Beta0*Beta1) Plot, warning=FALSE}
logbeta_plot = weather_bootstrap %>%
  mutate(results = map(models, broom::tidy)) %>%
  select(results) %>%
  unnest(results) %>%
  select(term, estimate) %>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate") %>%
  unnest() %>%
  janitor::clean_names() %>%
  mutate(intercept_beta1 = intercept*tmin,
         log_intercept_beta1 = log(intercept_beta1)) %>%
  ggplot(aes(x = log_intercept_beta1)) +
    geom_density() +
    labs(
      title = "Distribution of Beta0*Beta1",
      x = "Log(Beta0*Beta1)",
      y = "Density") +
    viridis::scale_color_viridis(discrete = TRUE) +
    theme(plot.title = element_text(hjust = 0.5))
logbeta_plot
```

The density plot for the distribution of the log(β̂ 0∗β̂ 1 ) of the linear regression model appears to be normally distributed as it is fairly symmetrical around 2.02, with minimal or no apparent skewness. The peak of the distribution is slightly uneven. 

### 95% CI of log(β̂ 0∗β̂ 1)

```{r Log(Beta0*Beta1) Confidence Interval, warning=FALSE}
weather_bootstrap %>%
  mutate(results = map(models, broom::tidy)) %>%
  select(results) %>%
  unnest(results) %>%
  select(term, estimate) %>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate") %>%
  unnest() %>%
  janitor::clean_names() %>%
  mutate(intercept_beta1 = intercept*tmin,
    log_intercept_beta1 = log(intercept_beta1)) %>%
  pull(log_intercept_beta1) %>%
  as.vector() %>%
  quantile(probs = c(0.025, 0.975), na.rm = TRUE)
```

The 95% confidence interval for the log(β̂ 0∗β̂ 1) is (1.97, 2.06). In other words, we are 95% confident that the true value of log(β̂ 0∗β̂ 1) in the central park weather data is between 1.97 and 2.06.


## Distribution of r̂ 2

```{r R^2 Plot}
r_squared_plot = weather_bootstrap %>%
  mutate(results = map(models, broom::glance)) %>%
  select(results) %>%
  unnest(results) %>%
  ggplot(aes(x = r.squared)) +
    geom_density() +
    labs(
      title = "Distribution of R^2",
      x = "R^2",
      y = "Density") +
    viridis::scale_color_viridis(discrete = TRUE) +
    theme(plot.title = element_text(hjust = 0.5))
r_squared_plot
```

The r-squared value is a statistical measure of how close the data follows the fitted regression line, or in other words, the percentage of variation in the the outcome variable that is explained by the model. The density plot for the distribution of the r-squared measures is fairly normally distributed, with slight left-skewedness, around an r-squared value of 0.9125. Because the density plot distribution is closer to 1 than 0, this indicates that the model explains much of the variability of the outcome data around its mean. 

### 95% CI of r̂ 2

```{r R^2 Plot Confidence Interval}
weather_bootstrap %>%
  mutate(
    results = map(models, broom::glance)) %>%
  select(results) %>%
  unnest(results) %>%
  pull(r.squared) %>%
  as.vector() %>%
  quantile(probs = c(0.025, 0.975), na.rm = TRUE)
```

The 95% confidence interval for the "r.squared" value is (0.89, 0.93). In other words, we are 95% confident that the true value of r-squared in the central park weather data is between 0.89 and 0.93.
